{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "data = com.ibm.analytics.dsxCoreUtils.DataUtil@77169f4e\n",
       "pc = ProjectContext(org.apache.spark.SparkContext@82fa750d,new-notebook,nb,Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJ1c2VybmFtZSI6ImRldnVzciIsInJvbGUiOiJzeXNhZG0iLCJ1aWQiOiI1MDAwIiwiaWF0IjoxNTg5NDEwMjczLCJleHAiOjE1ODk0NTcwNzN9.2z43Q4W2CsmPXDP2xwldFyb4pWyZtWAQ-_cQ9gljjTxJPOOIuLBx3fTseo0gYLYcx-51y8FRrXwJNsBB_NnEDRzC5F6AZwNLDTRyhrBZlP7UTuzPOONhzsP6HksRLGQMi8-bKHRUBJjWSbNReT1tqDTrLVA5CRalE_qwNEZKuDYwXjeICjfO4rvW1FqqR_RDA4lo-2ya_FteoUJyKUtvLCadCl4BAD3PUVKg7JtzAvqm0p9X0ZnxWkq1s1YjWuNJobyPa6mO6ATZVrXEQXoU1iIwGeh_2xmBuzo-w_Zcup-JgAtby4lKIxd5wEF6aVK0tV3r0m3IHU1kQqJ2Y4-D0g,9.30.96.34)\n",
       "projectName = new-...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "new-..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import com.ibm.analytics.dsxCoreUtils._\n",
    "val data = new DataUtil()\n",
    "// pc context contains projectName, notebookName, authToken, repositoryIp\n",
    "val pc = data.newProjectContext(sc, \"new-notebook\", \"nb\", \"Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJ1c2VybmFtZSI6ImRldnVzciIsInJvbGUiOiJzeXNhZG0iLCJ1aWQiOiI1MDAwIiwiaWF0IjoxNTg5NDEwMjczLCJleHAiOjE1ODk0NTcwNzN9.2z43Q4W2CsmPXDP2xwldFyb4pWyZtWAQ-_cQ9gljjTxJPOOIuLBx3fTseo0gYLYcx-51y8FRrXwJNsBB_NnEDRzC5F6AZwNLDTRyhrBZlP7UTuzPOONhzsP6HksRLGQMi8-bKHRUBJjWSbNReT1tqDTrLVA5CRalE_qwNEZKuDYwXjeICjfO4rvW1FqqR_RDA4lo-2ya_FteoUJyKUtvLCadCl4BAD3PUVKg7JtzAvqm0p9X0ZnxWkq1s1YjWuNJobyPa6mO6ATZVrXEQXoU1iIwGeh_2xmBuzo-w_Zcup-JgAtby4lKIxd5wEF6aVK0tV3r0m3IHU1kQqJ2Y4-D0g\", \"9.30.96.34\")\n",
    "\n",
    "// The projectName is the current project name\n",
    "val projectName = pc.projectName\n",
    "\n",
    "// The notebookName is the current notebook name\n",
    "val notebookName = pc.nbName\n",
    "\n",
    "// The authToken is the token generated by user management, which can access the backend service run\n",
    "val authToken = pc.authToken\n",
    "\n",
    "// The metaService is the backend service with https endpoint\n",
    "val metaService = \"https://\" + pc.repositoryIp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "db2driver = com.ibm.db2.jcc.DB2Driver\n",
       "db2_host = 9.30.128.37\n",
       "ddf_port = 8010\n",
       "db2_location = DB1B\n",
       "userid = mlin\n",
       "password = dev2017\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "dev2017"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val db2driver = \"com.ibm.db2.jcc.DB2Driver\";\n",
    "val db2_host = \"9.30.128.37\"; \n",
    "val ddf_port = \"8010\";   \n",
    "val db2_location = \"DB1B\";  \n",
    "val userid = \"mlin\";\n",
    "val password = \"dev2017\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "url = jdbc:db2://9.30.128.37:8010/DB1B\n",
       "connection = com.ibm.db2.jcc.t4.b@66e96523\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "com.ibm.db2.jcc.t4.b@66e96523"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.sql.{Connection, DriverManager, ResultSet};\n",
    "java.sql.DriverManager.registerDriver(new com.ibm.db2.jcc.DB2Driver);\n",
    "\n",
    "val url = List(\"jdbc:db2://\", db2_host, \":\", ddf_port, \"/\", db2_location).mkString(\"\");\n",
    "\n",
    "val connection = java.sql.DriverManager.getConnection(url, userid, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqlContext = org.apache.spark.sql.SQLContext@bda45b57\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SQLContext@bda45b57"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SQLContext\n",
    "import java.util.Properties\n",
    "val sqlContext = new SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "connectionProperties = {specialRegisters=CURRENT QUERY ACCELERATION=ALL, user=mlin, password=dev2017}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res82: Object = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{specialRegisters=CURRENT QUERY ACCELERATION=ALL, user=mlin, password=dev2017}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val connectionProperties = new Properties();\n",
    "\n",
    "connectionProperties.put(\"user\", userid);\n",
    "connectionProperties.put(\"password\", password);\n",
    "\n",
    "connectionProperties.put(\"specialRegisters\", \"CURRENT QUERY ACCELERATION=ALL\");\n",
    "//connectionProperties.put(\"specialRegisters\", \"CURRENT ACCELERATOR=IDAANOD,CURRENT QUERY ACCELERATION=ALL\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+---+---------+\n",
      "|AGE|ACTIVITY|STATE|SEX|NEGTWEETS|\n",
      "+---+--------+-----+---+---------+\n",
      "| 34|       1|   CA|  F|       11|\n",
      "| 61|       2|   CA|  F|        3|\n",
      "| 31|       2|   CA|  F|       10|\n",
      "| 30|       3|   CA|  F|       11|\n",
      "| 45|       1|   CA|  F|        7|\n",
      "| 22|       3|   CA|  F|       14|\n",
      "| 44|       3|   CA|  F|       14|\n",
      "| 44|       1|   CA|  F|        2|\n",
      "| 23|       1|   CA|  F|        5|\n",
      "| 47|       5|   CA|  F|       10|\n",
      "| 43|       1|   CA|  F|       10|\n",
      "| 81|       5|   CA|  F|        4|\n",
      "| 53|       3|   CA|  F|        3|\n",
      "| 60|       1|   CA|  F|        6|\n",
      "| 28|       0|   CA|  F|       14|\n",
      "| 68|       0|   CA|  F|        9|\n",
      "| 30|       4|   CA|  F|       11|\n",
      "| 30|       1|   CA|  F|        2|\n",
      "| 25|       2|   CA|  F|        1|\n",
      "| 36|       3|   CA|  F|        2|\n",
      "+---+--------+-----+---+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [AGE: int, ACTIVITY: int ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[AGE: int, ACTIVITY: int ... 3 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.jdbc(url,\n",
    "   \"(SELECT AGE, ACTIVITY, STATE, SEX, NEGTWEETS FROM SA.CUST_SUM WHERE STATE ='CA' and SEX='F') as t\",\n",
    "\n",
    "//   \"(SELECT \\\"C_NAME\\\",\\\"O_ORDERSTATUS\\\" FROM \\\"TPCH30M_E\\\".\\\"ORDERS\\\" INNER JOIN \\\"TPCH30M_E\\\".\\\"CUSTOMER\\\" ON \\\"O_CUSTKEY\\\"=\\\"C_CUSTKEY\\\" WHERE \\\"O_ORDERSTATUS\\\"='F' ) as t\",\n",
    "\n",
    "   connectionProperties)\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "createstmt = CREATE TABLE MLIN.AOT2 (C1 VARCHAR(1024), C2 VARCHAR(1024), C3 VARCHAR(1024),C4 VARCHAR(1024),C5 VARCHAR(1024)) IN ACCELERATOR IDAA3NOD CCSID EBCDIC\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val createstmt=\"CREATE TABLE MLIN.AOT2 (C1 VARCHAR(1024), C2 VARCHAR(1024), C3 VARCHAR(1024),C4 VARCHAR(1024),C5 VARCHAR(1024)) IN ACCELERATOR IDAA3NOD CCSID EBCDIC\";\n",
    "connection.createStatement.executeUpdate(createstmt)\n",
    " \n",
    "//val dropstmt=\"DROP TABLE MLIN.AOT2\";\n",
    "//connection.createStatement.executeUpdate(dropstmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+\n",
      "| C1| C2| C3| C4| C5|\n",
      "+---+---+---+---+---+\n",
      "| 44|  1| CA|  F|  2|\n",
      "| 23|  1| CA|  F|  5|\n",
      "| 78|  0| CA|  F|  4|\n",
      "| 50|  4| CA|  F|  3|\n",
      "| 74|  2| CA|  F|  2|\n",
      "| 43|  3| CA|  F|  4|\n",
      "| 52|  0| CA|  F|  5|\n",
      "| 44|  1| CA|  F|  4|\n",
      "| 36|  3| CA|  F|  2|\n",
      "| 62|  4| CA|  F|  3|\n",
      "| 33|  3| CA|  F|  8|\n",
      "| 33|  2| CA|  F|  3|\n",
      "| 47|  5| CA|  F| 10|\n",
      "| 45|  3| CA|  F|  2|\n",
      "| 65|  3| CA|  F|  2|\n",
      "| 24|  0| CA|  F| 10|\n",
      "| 37|  0| CA|  F| 12|\n",
      "| 45|  1| CA|  F| 14|\n",
      "| 31|  0| CA|  F| 11|\n",
      "| 43|  1| CA|  F| 10|\n",
      "+---+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [C1: string, C2: string ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[C1: string, C2: string ... 3 more fields]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.format(\"csv\").option(\"header\", \"false\").load(\"/mlzdata/devusr-iml-home/imlhome/tmp/STofCA.csv\").toDF(\"C1\",\"C2\",\"C3\",\"C4\",\"C5\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+\n",
      "| C1| C2| C3| C4| C5|\n",
      "+---+---+---+---+---+\n",
      "| 44|  1| CA|  F|  2|\n",
      "| 23|  1| CA|  F|  5|\n",
      "| 78|  0| CA|  F|  4|\n",
      "| 50|  4| CA|  F|  3|\n",
      "| 74|  2| CA|  F|  2|\n",
      "| 43|  3| CA|  F|  4|\n",
      "| 52|  0| CA|  F|  5|\n",
      "| 44|  1| CA|  F|  4|\n",
      "| 36|  3| CA|  F|  2|\n",
      "| 62|  4| CA|  F|  3|\n",
      "| 33|  3| CA|  F|  8|\n",
      "| 33|  2| CA|  F|  3|\n",
      "| 47|  5| CA|  F| 10|\n",
      "| 45|  3| CA|  F|  2|\n",
      "| 65|  3| CA|  F|  2|\n",
      "| 24|  0| CA|  F| 10|\n",
      "| 37|  0| CA|  F| 12|\n",
      "| 45|  1| CA|  F| 14|\n",
      "| 31|  0| CA|  F| 11|\n",
      "| 43|  1| CA|  F| 10|\n",
      "+---+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.{SaveMode, SparkSession}\n",
    "\n",
    "df.write.mode(SaveMode.Overwrite)\n",
    "  .format(\"jdbc\")\n",
    "  .option(\"url\", \"jdbc:db2://9.30.128.37:8010/DB1B\")\n",
    "  .option(\"dbtable\", \"MLIN.AOT2\")\n",
    "  .option(\"user\", \"mlin\")\n",
    "  .option(\"password\", \"dev2017\")\n",
    "  .save()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " C1 = 44 C2 = 1 C3 = CA C4 = F C5 = 2\n",
      " C1 = 23 C2 = 1 C3 = CA C4 = F C5 = 5\n",
      " C1 = 78 C2 = 0 C3 = CA C4 = F C5 = 4\n",
      " C1 = 50 C2 = 4 C3 = CA C4 = F C5 = 3\n",
      " C1 = 74 C2 = 2 C3 = CA C4 = F C5 = 2\n",
      " C1 = 43 C2 = 3 C3 = CA C4 = F C5 = 4\n",
      " C1 = 52 C2 = 0 C3 = CA C4 = F C5 = 5\n",
      " C1 = 44 C2 = 1 C3 = CA C4 = F C5 = 4\n",
      " C1 = 36 C2 = 3 C3 = CA C4 = F C5 = 2\n",
      " C1 = 62 C2 = 4 C3 = CA C4 = F C5 = 3\n",
      " C1 = 33 C2 = 3 C3 = CA C4 = F C5 = 8\n",
      " C1 = 33 C2 = 2 C3 = CA C4 = F C5 = 3\n",
      " C1 = 47 C2 = 5 C3 = CA C4 = F C5 = 10\n",
      " C1 = 45 C2 = 3 C3 = CA C4 = F C5 = 2\n",
      " C1 = 65 C2 = 3 C3 = CA C4 = F C5 = 2\n",
      " C1 = 24 C2 = 0 C3 = CA C4 = F C5 = 10\n",
      " C1 = 37 C2 = 0 C3 = CA C4 = F C5 = 12\n",
      " C1 = 45 C2 = 1 C3 = CA C4 = F C5 = 14\n",
      " C1 = 31 C2 = 0 C3 = CA C4 = F C5 = 11\n",
      " C1 = 43 C2 = 1 C3 = CA C4 = F C5 = 10\n",
      " C1 = 39 C2 = 2 C3 = CA C4 = F C5 = 9\n",
      " C1 = 46 C2 = 2 C3 = CA C4 = F C5 = 2\n",
      " C1 = 31 C2 = 2 C3 = CA C4 = F C5 = 13\n",
      " C1 = 61 C2 = 2 C3 = CA C4 = F C5 = 3\n",
      " C1 = 40 C2 = 4 C3 = CA C4 = F C5 = 3\n",
      " C1 = 45 C2 = 2 C3 = CA C4 = F C5 = 3\n",
      " C1 = 41 C2 = 4 C3 = CA C4 = F C5 = 4\n",
      " C1 = 63 C2 = 1 C3 = CA C4 = F C5 = 11\n",
      " C1 = 43 C2 = 3 C3 = CA C4 = F C5 = 11\n",
      " C1 = 30 C2 = 2 C3 = CA C4 = F C5 = 9\n",
      " C1 = 35 C2 = 0 C3 = CA C4 = F C5 = 9\n",
      " C1 = 23 C2 = 1 C3 = CA C4 = F C5 = 10\n",
      " C1 = 83 C2 = 3 C3 = CA C4 = F C5 = 3\n",
      " C1 = 62 C2 = 2 C3 = CA C4 = F C5 = 15\n",
      " C1 = 23 C2 = 3 C3 = CA C4 = F C5 = 1\n",
      " C1 = 62 C2 = 4 C3 = CA C4 = F C5 = 3\n",
      " C1 = 44 C2 = 1 C3 = CA C4 = F C5 = 11\n",
      " C1 = 35 C2 = 2 C3 = CA C4 = F C5 = 3\n",
      " C1 = 30 C2 = 5 C3 = CA C4 = F C5 = 12\n",
      " C1 = 20 C2 = 2 C3 = CA C4 = F C5 = 15\n",
      " C1 = 32 C2 = 1 C3 = CA C4 = F C5 = 12\n",
      " C1 = 30 C2 = 0 C3 = CA C4 = F C5 = 9\n",
      " C1 = 25 C2 = 2 C3 = CA C4 = F C5 = 9\n",
      " C1 = 55 C2 = 1 C3 = CA C4 = F C5 = 5\n",
      " C1 = 44 C2 = 1 C3 = CA C4 = F C5 = 8\n",
      " C1 = 70 C2 = 1 C3 = CA C4 = F C5 = 13\n",
      " C1 = 35 C2 = 0 C3 = CA C4 = F C5 = 11\n",
      " C1 = 30 C2 = 4 C3 = CA C4 = F C5 = 2\n",
      " C1 = 31 C2 = 2 C3 = CA C4 = F C5 = 10\n",
      " C1 = 42 C2 = 4 C3 = CA C4 = F C5 = 3\n",
      " C1 = 56 C2 = 5 C3 = CA C4 = F C5 = 7\n",
      " C1 = 36 C2 = 1 C3 = CA C4 = F C5 = 14\n",
      " C1 = 33 C2 = 1 C3 = CA C4 = F C5 = 9\n",
      " C1 = 41 C2 = 1 C3 = CA C4 = F C5 = 3\n",
      " C1 = 30 C2 = 2 C3 = CA C4 = F C5 = 10\n",
      " C1 = 35 C2 = 1 C3 = CA C4 = F C5 = 13\n",
      " C1 = 28 C2 = 2 C3 = CA C4 = F C5 = 6\n",
      " C1 = 39 C2 = 2 C3 = CA C4 = F C5 = 3\n",
      " C1 = 38 C2 = 1 C3 = CA C4 = F C5 = 11\n",
      " C1 = 69 C2 = 2 C3 = CA C4 = F C5 = 9\n",
      " C1 = 81 C2 = 5 C3 = CA C4 = F C5 = 4\n",
      " C1 = 44 C2 = 1 C3 = CA C4 = F C5 = 8\n",
      " C1 = 30 C2 = 3 C3 = CA C4 = F C5 = 11\n",
      " C1 = 38 C2 = 1 C3 = CA C4 = F C5 = 11\n",
      " C1 = 60 C2 = 1 C3 = CA C4 = F C5 = 2\n",
      " C1 = 50 C2 = 1 C3 = CA C4 = F C5 = 3\n",
      " C1 = 34 C2 = 0 C3 = CA C4 = F C5 = 11\n",
      " C1 = 48 C2 = 1 C3 = CA C4 = F C5 = 4\n",
      " C1 = 30 C2 = 2 C3 = CA C4 = F C5 = 10\n",
      " C1 = 75 C2 = 2 C3 = CA C4 = F C5 = 2\n",
      " C1 = 37 C2 = 0 C3 = CA C4 = F C5 = 12\n",
      " C1 = 34 C2 = 1 C3 = CA C4 = F C5 = 14\n",
      " C1 = 47 C2 = 2 C3 = CA C4 = F C5 = 2\n",
      " C1 = 45 C2 = 1 C3 = CA C4 = F C5 = 9\n",
      " C1 = 32 C2 = 4 C3 = CA C4 = F C5 = 3\n",
      " C1 = 27 C2 = 4 C3 = CA C4 = F C5 = 8\n",
      " C1 = 56 C2 = 3 C3 = CA C4 = F C5 = 4\n",
      " C1 = 30 C2 = 4 C3 = CA C4 = F C5 = 11\n",
      " C1 = 45 C2 = 1 C3 = CA C4 = F C5 = 7\n",
      " C1 = 55 C2 = 3 C3 = CA C4 = F C5 = 6\n",
      " C1 = 65 C2 = 5 C3 = CA C4 = F C5 = 11\n",
      " C1 = 22 C2 = 3 C3 = CA C4 = F C5 = 14\n",
      " C1 = 53 C2 = 3 C3 = CA C4 = F C5 = 3\n",
      " C1 = 23 C2 = 0 C3 = CA C4 = F C5 = 17\n",
      " C1 = 30 C2 = 1 C3 = CA C4 = F C5 = 2\n",
      " C1 = 35 C2 = 3 C3 = CA C4 = F C5 = 4\n",
      " C1 = 43 C2 = 0 C3 = CA C4 = F C5 = 11\n",
      " C1 = 34 C2 = 0 C3 = CA C4 = F C5 = 10\n",
      " C1 = 49 C2 = 1 C3 = CA C4 = F C5 = 16\n",
      " C1 = 30 C2 = 4 C3 = CA C4 = F C5 = 4\n",
      " C1 = 29 C2 = 1 C3 = CA C4 = F C5 = 12\n",
      " C1 = 27 C2 = 2 C3 = CA C4 = F C5 = 3\n",
      " C1 = 34 C2 = 2 C3 = CA C4 = F C5 = 3\n",
      " C1 = 21 C2 = 2 C3 = CA C4 = F C5 = 6\n",
      " C1 = 30 C2 = 3 C3 = CA C4 = F C5 = 13\n",
      " C1 = 60 C2 = 1 C3 = CA C4 = F C5 = 6\n",
      " C1 = 23 C2 = 4 C3 = CA C4 = F C5 = 13\n",
      " C1 = 62 C2 = 0 C3 = CA C4 = F C5 = 4\n",
      " C1 = 34 C2 = 0 C3 = CA C4 = F C5 = 11\n",
      " C1 = 52 C2 = 2 C3 = CA C4 = F C5 = 9\n",
      " C1 = 27 C2 = 2 C3 = CA C4 = F C5 = 5\n",
      " C1 = 27 C2 = 2 C3 = CA C4 = F C5 = 13\n",
      " C1 = 58 C2 = 4 C3 = CA C4 = F C5 = 2\n",
      " C1 = 71 C2 = 0 C3 = CA C4 = F C5 = 6\n",
      " C1 = 43 C2 = 0 C3 = CA C4 = F C5 = 8\n",
      " C1 = 76 C2 = 2 C3 = CA C4 = F C5 = 7\n",
      " C1 = 64 C2 = 2 C3 = CA C4 = F C5 = 10\n",
      " C1 = 41 C2 = 1 C3 = CA C4 = F C5 = 12\n",
      " C1 = 28 C2 = 1 C3 = CA C4 = F C5 = 10\n",
      " C1 = 33 C2 = 1 C3 = CA C4 = F C5 = 4\n",
      " C1 = 28 C2 = 1 C3 = CA C4 = F C5 = 10\n",
      " C1 = 20 C2 = 0 C3 = CA C4 = F C5 = 7\n",
      " C1 = 34 C2 = 1 C3 = CA C4 = F C5 = 5\n",
      " C1 = 57 C2 = 0 C3 = CA C4 = F C5 = 11\n",
      " C1 = 43 C2 = 0 C3 = CA C4 = F C5 = 3\n",
      " C1 = 34 C2 = 0 C3 = CA C4 = F C5 = 6\n",
      " C1 = 85 C2 = 4 C3 = CA C4 = F C5 = 2\n",
      " C1 = 76 C2 = 2 C3 = CA C4 = F C5 = 5\n",
      " C1 = 33 C2 = 5 C3 = CA C4 = F C5 = 2\n",
      " C1 = 28 C2 = 0 C3 = CA C4 = F C5 = 14\n",
      " C1 = 24 C2 = 2 C3 = CA C4 = F C5 = 6\n",
      " C1 = 44 C2 = 3 C3 = CA C4 = F C5 = 14\n",
      " C1 = 59 C2 = 2 C3 = CA C4 = F C5 = 1\n",
      " C1 = 68 C2 = 2 C3 = CA C4 = F C5 = 11\n",
      " C1 = 68 C2 = 4 C3 = CA C4 = F C5 = 3\n",
      " C1 = 35 C2 = 1 C3 = CA C4 = F C5 = 6\n",
      " C1 = 68 C2 = 0 C3 = CA C4 = F C5 = 9\n",
      " C1 = 41 C2 = 1 C3 = CA C4 = F C5 = 14\n",
      " C1 = 60 C2 = 3 C3 = CA C4 = F C5 = 9\n",
      " C1 = 33 C2 = 4 C3 = CA C4 = F C5 = 4\n",
      " C1 = 30 C2 = 2 C3 = CA C4 = F C5 = 5\n",
      " C1 = 46 C2 = 1 C3 = CA C4 = F C5 = 4\n",
      " C1 = 31 C2 = 0 C3 = CA C4 = F C5 = 8\n",
      " C1 = 23 C2 = 4 C3 = CA C4 = F C5 = 3\n",
      " C1 = 54 C2 = 3 C3 = CA C4 = F C5 = 2\n",
      " C1 = 21 C2 = 3 C3 = CA C4 = F C5 = 10\n",
      " C1 = 39 C2 = 1 C3 = CA C4 = F C5 = 14\n",
      " C1 = 30 C2 = 4 C3 = CA C4 = F C5 = 3\n",
      " C1 = 34 C2 = 1 C3 = CA C4 = F C5 = 11\n",
      " C1 = 25 C2 = 2 C3 = CA C4 = F C5 = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "querystmt = SELECT * FROM MLIN.AOT2\n",
       "result = com.ibm.db2.jcc.t4.h@a851bac\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "com.ibm.db2.jcc.t4.h@a851bac"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//val queryacc = \"SELECT \\\"O_ORDERKEY\\\" FROM \\\"TPCH30M_E\\\".\\\"ORDERS\\\"\";\n",
    "//Select from an AOT table\n",
    "val querystmt=\"SELECT * FROM MLIN.AOT2\";\n",
    "val result = connection.createStatement.executeQuery(querystmt)\n",
    "\n",
    "\n",
    "while ( result.next() ) {\n",
    "    val col1 = result.getString(\"C1\")\n",
    "    val col2 = result.getString(\"C2\")\n",
    "    val col3 = result.getString(\"C3\")\n",
    "    val col4 = result.getString(\"C4\")\n",
    "    val col5 = result.getString(\"C5\")\n",
    "    print(\" C1 = \" + col1)\n",
    "    print(\" C2 = \" + col2)\n",
    "    print(\" C3 = \" + col3)\n",
    "    print(\" C4 = \" + col4)\n",
    "    println(\" C5 = \" + col5)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+---+---------+\n",
      "|AGE|ACTIVITY|STATE|SEX|NEGTWEETS|\n",
      "+---+--------+-----+---+---------+\n",
      "| 44|       1|   CA|  F|        2|\n",
      "| 23|       1|   CA|  F|        5|\n",
      "| 78|       0|   CA|  F|        4|\n",
      "| 50|       4|   CA|  F|        3|\n",
      "| 74|       2|   CA|  F|        2|\n",
      "+---+--------+-----+---+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataRaw = [AGE: int, ACTIVITY: int ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[AGE: int, ACTIVITY: int ... 3 more fields]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._ \n",
    "\n",
    "//Load data from DB2 for z/OS using JDBC driver\n",
    "val DataRaw = spark.read.format(\"jdbc\").\n",
    "                        options(Map(\"driver\" -> \"com.ibm.db2.jcc.DB2Driver\",\n",
    "                         \"url\" -> \"jdbc:db2://9.30.128.37:8010/DB1B\", \n",
    "                         \"user\" -> \"mlzfvt\",\"password\" -> \"whoopi\", \n",
    "                         \"dbtable\" -> \"(SELECT AGE, ACTIVITY, STATE, SEX, NEGTWEETS FROM SA.CUST_SUM WHERE STATE ='CA' and SEX='F') T\")).load()\n",
    "\n",
    "DataRaw.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.sql.AnalysisException\n",
       "Message: path file:/mlzdata/devusr-iml-home/imlhome/tmp/STofCA.csv already exists.;\n",
       "StackTrace:   at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:114)\n",
       "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
       "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
       "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
       "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
       "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
       "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
       "  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n",
       "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
       "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n",
       "  at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:664)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataRaw.write.csv(\"/mlzdata/devusr-iml-home/imlhome/tmp/STofCA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+\n",
      "|_c0|_c1|_c2|_c3|_c4|\n",
      "+---+---+---+---+---+\n",
      "| 44|  1| CA|  F|  2|\n",
      "| 23|  1| CA|  F|  5|\n",
      "| 78|  0| CA|  F|  4|\n",
      "| 50|  4| CA|  F|  3|\n",
      "| 74|  2| CA|  F|  2|\n",
      "| 43|  3| CA|  F|  4|\n",
      "| 52|  0| CA|  F|  5|\n",
      "| 44|  1| CA|  F|  4|\n",
      "| 36|  3| CA|  F|  2|\n",
      "| 62|  4| CA|  F|  3|\n",
      "| 33|  3| CA|  F|  8|\n",
      "| 33|  2| CA|  F|  3|\n",
      "| 47|  5| CA|  F| 10|\n",
      "| 45|  3| CA|  F|  2|\n",
      "| 65|  3| CA|  F|  2|\n",
      "| 24|  0| CA|  F| 10|\n",
      "| 37|  0| CA|  F| 12|\n",
      "| 45|  1| CA|  F| 14|\n",
      "| 31|  0| CA|  F| 11|\n",
      "| 43|  1| CA|  F| 10|\n",
      "+---+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+\n",
      "|_c0|\n",
      "+---+\n",
      "| 44|\n",
      "| 23|\n",
      "| 78|\n",
      "| 50|\n",
      "| 74|\n",
      "| 43|\n",
      "| 52|\n",
      "| 44|\n",
      "| 36|\n",
      "| 62|\n",
      "| 33|\n",
      "| 33|\n",
      "| 47|\n",
      "| 45|\n",
      "| 65|\n",
      "| 24|\n",
      "| 37|\n",
      "| 45|\n",
      "| 31|\n",
      "| 43|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [_c0: string, _c1: string ... 3 more fields]\n",
       "sqlDF1 = [_c0: string, _c1: string ... 3 more fields]\n",
       "sqlDF2 = [_c0: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[_c0: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.csv(\"/mlzdata/devusr-iml-home/imlhome/tmp/STofCA.csv\")\n",
    "df.createOrReplaceTempView(\"churnTable\")\n",
    "\n",
    "val sqlDF1 = spark.sql(\"SELECT * FROM churnTable\")\n",
    "sqlDF1.show()\n",
    "\n",
    "val sqlDF2 = spark.sql(\"SELECT _c0 FROM churnTable\")\n",
    "sqlDF2.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df2 = [_c0: string, _c1: string ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[_c0: string, _c1: string ... 3 more fields]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.{SaveMode, SparkSession}\n",
    "\n",
    "val df2 = spark.read.csv(\"/mlzdata/devusr-iml-home/imlhome/tmp/STofCA.csv\")\n",
    "df2.write.mode(SaveMode.Overwrite)\n",
    "  .format(\"jdbc\")\n",
    "  .option(\"url\", \"jdbc:db2://9.30.128.37:8010/DB1B\")\n",
    "  .option(\"dbtable\", \"MLIN.STOFCA\")\n",
    "  .option(\"user\", \"mlin\")\n",
    "  .option(\"password\", \"dev2017\")\n",
    "  .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 0 in stage 29.0 failed 4 times, most recent failure: Lost task 0.3 in stage 29.0 (TID 32, 9.30.128.37, executor 0): com.ibm.db2.jcc.am.SqlException: DB2 SQL Error: SQLCODE=-4742, SQLSTATE=560D5, SQLERRMC=25, DRIVER=4.25.13\n",
       "\tat com.ibm.db2.jcc.am.b6.a(b6.java:815)\n",
       "\tat com.ibm.db2.jcc.am.b6.a(b6.java:66)\n",
       "\tat com.ibm.db2.jcc.am.b6.a(b6.java:140)\n",
       "\tat com.ibm.db2.jcc.am.k3.c(k3.java:2824)\n",
       "\tat com.ibm.db2.jcc.am.k3.d(k3.java:2808)\n",
       "\tat com.ibm.db2.jcc.am.k3.a(k3.java:2234)\n",
       "\tat com.ibm.db2.jcc.am.k4.a(k4.java:8242)\n",
       "\tat com.ibm.db2.jcc.am.k3.a(k3.java:2210)\n",
       "\tat com.ibm.db2.jcc.t4.ab.i(ab.java:201)\n",
       "\tat com.ibm.db2.jcc.t4.ab.b(ab.java:96)\n",
       "\tat com.ibm.db2.jcc.t4.p.a(p.java:32)\n",
       "\tat com.ibm.db2.jcc.t4.av.i(av.java:150)\n",
       "\tat com.ibm.db2.jcc.am.k3.al(k3.java:2203)\n",
       "\tat com.ibm.db2.jcc.am.k4.bq(k4.java:3730)\n",
       "\tat com.ibm.db2.jcc.am.k4.a(k4.java:4609)\n",
       "\tat com.ibm.db2.jcc.am.k4.b(k4.java:4182)\n",
       "\tat com.ibm.db2.jcc.am.k4.bd(k4.java:780)\n",
       "\tat com.ibm.db2.jcc.am.k4.executeQuery(k4.java:745)\n",
       "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:304)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1459)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1160)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
       "\tat java.lang.Thread.run(Thread.java:812)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: \tat com.ibm.db2.jcc.am.b6.a(b6.java:815)\n",
       "\tat com.ibm.db2.jcc.am.b6.a(b6.java:66)\n",
       "\tat com.ibm.db2.jcc.am.b6.a(b6.java:140)\n",
       "\tat com.ibm.db2.jcc.am.k3.c(k3.java:2824)\n",
       "\tat com.ibm.db2.jcc.am.k3.d(k3.java:2808)\n",
       "\tat com.ibm.db2.jcc.am.k3.a(k3.java:2234)\n",
       "\tat com.ibm.db2.jcc.am.k4.a(k4.java:8242)\n",
       "\tat com.ibm.db2.jcc.am.k3.a(k3.java:2210)\n",
       "\tat com.ibm.db2.jcc.t4.ab.i(ab.java:201)\n",
       "\tat com.ibm.db2.jcc.t4.ab.b(ab.java:96)\n",
       "\tat com.ibm.db2.jcc.t4.p.a(p.java:32)\n",
       "\tat com.ibm.db2.jcc.t4.av.i(av.java:150)\n",
       "\tat com.ibm.db2.jcc.am.k3.al(k3.java:2203)\n",
       "\tat com.ibm.db2.jcc.am.k4.bq(k4.java:3730)\n",
       "\tat com.ibm.db2.jcc.am.k4.a(k4.java:4609)\n",
       "\tat com.ibm.db2.jcc.am.k4.b(k4.java:4182)\n",
       "\tat com.ibm.db2.jcc.am.k4.bd(k4.java:780)\n",
       "\tat com.ibm.db2.jcc.am.k4.executeQuery(k4.java:745)\n",
       "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:304)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1459)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1160)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
       "\tat java.lang.Thread.run(Thread.java:812)\n",
       "Driver stacktrace:\n",
       "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n",
       "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n",
       "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n",
       "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n",
       "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n",
       "  at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
       "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n",
       "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n",
       "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n",
       "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n",
       "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n",
       "  at org.apache.spark.sql.Dataset.show(Dataset.scala:751)\n",
       "  at org.apache.spark.sql.Dataset.show(Dataset.scala:710)\n",
       "  at org.apache.spark.sql.Dataset.show(Dataset.scala:719)\n",
       "  ... 50 elided\n",
       "Caused by: com.ibm.db2.jcc.am.SqlException: DB2 SQL Error: SQLCODE=-4742, SQLSTATE=560D5, SQLERRMC=25, DRIVER=4.25.13\n",
       "  at com.ibm.db2.jcc.am.b6.a(b6.java:815)\n",
       "  at com.ibm.db2.jcc.am.b6.a(b6.java:66)\n",
       "  at com.ibm.db2.jcc.am.b6.a(b6.java:140)\n",
       "  at com.ibm.db2.jcc.am.k3.c(k3.java:2824)\n",
       "  at com.ibm.db2.jcc.am.k3.d(k3.java:2808)\n",
       "  at com.ibm.db2.jcc.am.k3.a(k3.java:2234)\n",
       "  at com.ibm.db2.jcc.am.k4.a(k4.java:8242)\n",
       "  at com.ibm.db2.jcc.am.k3.a(k3.java:2210)\n",
       "  at com.ibm.db2.jcc.t4.ab.i(ab.java:201)\n",
       "  at com.ibm.db2.jcc.t4.ab.b(ab.java:96)\n",
       "  at com.ibm.db2.jcc.t4.p.a(p.java:32)\n",
       "  at com.ibm.db2.jcc.t4.av.i(av.java:150)\n",
       "  at com.ibm.db2.jcc.am.k3.al(k3.java:2203)\n",
       "  at com.ibm.db2.jcc.am.k4.bq(k4.java:3730)\n",
       "  at com.ibm.db2.jcc.am.k4.a(k4.java:4609)\n",
       "  at com.ibm.db2.jcc.am.k4.b(k4.java:4182)\n",
       "  at com.ibm.db2.jcc.am.k4.bd(k4.java:780)\n",
       "  at com.ibm.db2.jcc.am.k4.executeQuery(k4.java:745)\n",
       "  at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:304)\n",
       "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
       "  at org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
       "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1459)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df3 = spark.read.format(\"jdbc\").\n",
    "                        options(Map(\"driver\" -> \"com.ibm.db2.jcc.DB2Driver\",\n",
    "                         \"url\" -> \"jdbc:db2://9.30.128.37:8010/DB1B:specialRegisters=CURRENT QUERY ACCELERATION=ALL;\", \n",
    "                         \"user\" -> \"mlzfvt\",\"password\" -> \"whoopi\", \n",
    "                         \"dbtable\" -> \"(SELECT * FROM MLIN.STOFCA) T\")).load()\n",
    "\n",
    "df3.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Name: com.ibm.db2.jcc.am.SqlSyntaxErrorException\n",
       "Message: DB2 SQL Error: SQLCODE=-104, SQLSTATE=42601, SQLERRMC=MLIN;JOIN CROSS INNER LEFT RIGHT FULL (, DRIVER=4.25.13\n",
       "StackTrace:   at com.ibm.db2.jcc.am.b6.a(b6.java:810)\n",
       "  at com.ibm.db2.jcc.am.b6.a(b6.java:66)\n",
       "  at com.ibm.db2.jcc.am.b6.a(b6.java:140)\n",
       "  at com.ibm.db2.jcc.am.k3.c(k3.java:2824)\n",
       "  at com.ibm.db2.jcc.am.k3.d(k3.java:2808)\n",
       "  at com.ibm.db2.jcc.am.k3.a(k3.java:2234)\n",
       "  at com.ibm.db2.jcc.am.k4.a(k4.java:8242)\n",
       "  at com.ibm.db2.jcc.am.k3.a(k3.java:2210)\n",
       "  at com.ibm.db2.jcc.t4.ab.i(ab.java:201)\n",
       "  at com.ibm.db2.jcc.t4.ab.b(ab.java:96)\n",
       "  at com.ibm.db2.jcc.t4.p.a(p.java:32)\n",
       "  at com.ibm.db2.jcc.t4.av.i(av.java:150)\n",
       "  at com.ibm.db2.jcc.am.k3.al(k3.java:2203)\n",
       "  at com.ibm.db2.jcc.am.k4.bq(k4.java:3730)\n",
       "  at com.ibm.db2.jcc.am.k4.a(k4.java:4609)\n",
       "  at com.ibm.db2.jcc.am.k4.b(k4.java:4182)\n",
       "  at com.ibm.db2.jcc.am.k4.bd(k4.java:780)\n",
       "  at com.ibm.db2.jcc.am.k4.executeQuery(k4.java:745)\n",
       "  at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:61)\n",
       "  at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:210)\n",
       "  at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:35)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:318)\n",
       "  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n",
       "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
       "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:167)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"jdbc\").\n",
    "        options(Map(\"driver\" -> \"com.ibm.db2.jcc.DB2Driver\",\n",
    "                    \"url\" -> \"jdbc:db2://9.30.128.37:8010/DB1B:specialRegisters=CURRENT QUERY ACCELERATION=ALL;\", \n",
    "                    \"user\" -> \"mlzfvt\",\"password\" -> \"whoopi\", \n",
    "                    \"dbtable\" -> \"(CREATE TABLE MLIN.AOT2 (C1 INT) IN ACCELERATOR IDAA3NOD) T\")).load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:33: error: not found: value ls\n",
       "       !ls -al\n",
       "        ^\n",
       "<console>:33: error: not found: value al\n",
       "       !ls -al\n",
       "            ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "component": "zed6e91m",
  "kernelspec": {
   "display_name": "imlspark - Scala",
   "language": "scala",
   "name": "imlspark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
